{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13395699,"sourceType":"datasetVersion","datasetId":8500602}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ahmadindragiri/mental-health-and-social-media-balance?scriptVersionId=270261142\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Mental Health and Social Media Balance â€” Data Exploration\n\n## Project Objective\nThe main goal of this project is to explore how social media usage may influence mental health conditions.  \nWe will use statistical analysis and machine learning to uncover potential patterns and predictive factors.\n## Dataset Overview\nThe dataset contains various indicators related to mental health and social media balance â€” such as time spent on social media, sleep quality, anxiety, depression level, and overall well-being.\n\nIn this phase, we will:\n1. Load and preview the dataset.\n2. Check missing values and data types.\n3. Understand key descriptive statistics.\n4. Visualize basic data distributions.\n### Research Questions\n- Is there a correlation between social media time and mental health level?\n- Which factors are most associated with stress or anxiety?\n- Can we identify early indicators of poor mental health using data patterns?\n### Libraries Used\n- `pandas` for data manipulation  \n- `numpy` for numerical operations  \n- `matplotlib` & `seaborn` for visualization  \n- `scipy` for statistical analysis","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:18.657951Z","iopub.execute_input":"2025-10-23T12:19:18.658623Z","iopub.status.idle":"2025-10-23T12:19:18.667126Z","shell.execute_reply.started":"2025-10-23T12:19:18.658598Z","shell.execute_reply":"2025-10-23T12:19:18.666358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Style for Visualizations\nsns.set(style=\"whitegrid\", palette=\"pastel\")\n\n# Load Dataset\ndf = pd.read_csv(\"/kaggle/input/mental-health-and-social-media-balance-dataset/Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n\n# Display first few rows\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:18.669018Z","iopub.execute_input":"2025-10-23T12:19:18.669892Z","iopub.status.idle":"2025-10-23T12:19:18.704351Z","shell.execute_reply.started":"2025-10-23T12:19:18.669861Z","shell.execute_reply":"2025-10-23T12:19:18.703364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dataset info: \")\ndf.info()\n\nprint(\"n\\ Dataset Shape: \", df.shape)\n\n# Check Missing Value\nprint(\"n\\ Missing Values: \")\nprint(df.isnull().sum())\n\ndf.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:18.705686Z","iopub.execute_input":"2025-10-23T12:19:18.705938Z","iopub.status.idle":"2025-10-23T12:19:18.743329Z","shell.execute_reply.started":"2025-10-23T12:19:18.705921Z","shell.execute_reply":"2025-10-23T12:19:18.742488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of Numerical Column\nnumerical_cols = df.select_dtypes(include=np.number).columns\n\nplt.figure(figsize=(14,5))\ndf[numerical_cols].hist(bins=20, figsize=(14, 10), color='skyblue', edgecolor='black')\nplt.suptitle(\"DistributionofNumerical Features\", fontsize=18)\nplt.show()\n\n# Correlation\nplt.figure(figsize=(10, 6))\nsns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:18.744377Z","iopub.execute_input":"2025-10-23T12:19:18.744656Z","iopub.status.idle":"2025-10-23T12:19:20.838862Z","shell.execute_reply.started":"2025-10-23T12:19:18.744637Z","shell.execute_reply":"2025-10-23T12:19:20.837939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Summary and Initial Observations\n\n## Dataset Overview\n- **Total records:** 500  \n- **Total columns:** 10  \n- **No missing values** were detected.  \n- The dataset includes both **numerical** and **categorical** attributes, making it suitable for correlation analysis and model building.\n\n## Key Statistical Insights\n| Feature | Mean | Std | Min | Max | Observation |\n|----------|------|------|------|------|-------------|\n| Age | 32.99 | 9.96 | 16 | 49 | Most users are young to middle-aged adults. |\n| Daily_Screen_Time(hrs) | 5.53 | 1.73 | 1.0 | 10.8 | Average screen time is around 5.5 hours per day. |\n| Sleep_Quality(1-10) | 6.30 | 1.53 | 2 | 10 | Sleep quality tends to be moderate. |\n| Stress_Level(1-10) | 6.62 | 1.54 | 2 | 10 | Average stress level is relatively high. |\n| Days_Without_Social_Media | 3.13 | 1.86 | 0 | 9 | Many users can stay offline for 2â€“3 days. |\n| Exercise_Frequency(week) | 2.45 | 1.43 | 0 | 7 | Physical activity is relatively low. |\n| Happiness_Index(1-10) | 8.38 | 1.52 | 4 | 10 | Users tend to report high happiness scores overall. |\n\n## Preliminary Interpretation\n1. **No missing values** â†’ preprocessing will focus more on normalization and encoding rather than imputation.  \n2. There might be a **negative relationship** between screen time and sleep quality.  \n3. **Stress and happiness** could be inversely related.  \n4. **Exercise frequency** and **days without social media** may have positive effects on mental well-being.  ","metadata":{}},{"cell_type":"markdown","source":"# Statistical Analysis & Correlation\n\n## Goals\n- Quantify relationships between social media usage and mental health indicators.\n- Test hypotheses using appropriate statistical tests.\n- Produce visualizations that support inference.\n- Prepare features for subsequent modeling.\n\n## Outline\n1. Data preparation (rename, types, encode categorical variables if needed)\n2. Correlation analysis (Pearson correlation + heatmap)\n3. Pairwise scatterplots and regression lines\n4. Hypothesis testing:\n   - Pearson correlation significance\n   - T-tests (e.g., Gender differences)\n   - ANOVA (e.g., different platforms)\n5. Effect sizes and interpretation\n6. Feature selection guidance for modeling","metadata":{}},{"cell_type":"code","source":"# Statistical Library\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Machine Learning Helper\nfrom sklearn.preprocessing import LabelEncoder\n\ndata = df.copy()\n\n# Rename long column names to friendly variable names for code readability\ndata = data.rename(columns={\n    \"User_ID\": \"user_id\",\n    \"Age\": \"age\",\n    \"Gender\": \"gender\",\n    \"Daily_Screen_Time(hrs)\": \"screen_time\",\n    \"Sleep_Quality(1-10)\": \"sleep_quality\",\n    \"Stress_Level(1-10)\": \"stress_level\",\n    \"Days_Without_Social_Media\": \"days_off_sm\",\n    \"Exercise_Frequency(week)\": \"exercise_freq\",\n    \"Social_Media_Platform\": \"platform\",\n    \"Happiness_Index(1-10)\": \"happiness\"\n})\n\n# Quick sanity checks\nprint(\"Shape:\", data.shape)\nprint(\"Dtypes:\\n\", data.dtypes)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:20.840838Z","iopub.execute_input":"2025-10-23T12:19:20.841209Z","iopub.status.idle":"2025-10-23T12:19:20.860966Z","shell.execute_reply.started":"2025-10-23T12:19:20.841182Z","shell.execute_reply":"2025-10-23T12:19:20.860077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Notes\n- I renamed columns for convenience.\n- All variables appear numeric except `gender`, `platform`, and `user_id`.\n- We will drop `user_id` for analysis (non-informative identifier).\n","metadata":{}},{"cell_type":"code","source":"# Drop user id column for Statistical analysis\ndata = data.drop(columns=[\"user_id\"])\n\n# Convert categorical columns to dtype 'category' for convenience\ndata[\"gender\"] = data[\"gender\"].astype(\"category\")\ndata[\"platform\"] = data[\"platform\"].astype(\"category\")\n\ndata.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:20.861856Z","iopub.execute_input":"2025-10-23T12:19:20.862278Z","iopub.status.idle":"2025-10-23T12:19:20.891745Z","shell.execute_reply.started":"2025-10-23T12:19:20.862252Z","shell.execute_reply":"2025-10-23T12:19:20.890523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation matrix (Pearson) â€” numeric variables only\n\nI will compute Pearson correlation coefficients between numeric variables and test their significance.","metadata":{}},{"cell_type":"code","source":"# Select numeric columns\nnumeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols\n\n# Compute Pearson correlation matrix\ncorr = data[numeric_cols].corr(method=\"pearson\")\ncorr\n\n# Visualize correlation matrix with heatmap\nplt.figure(figsize=(9,7))\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0, linewidths=0.5)\nplt.title(\"Pearson Correlation Matrix (Numeric Features)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:20.892731Z","iopub.execute_input":"2025-10-23T12:19:20.893148Z","iopub.status.idle":"2025-10-23T12:19:21.31737Z","shell.execute_reply.started":"2025-10-23T12:19:20.893092Z","shell.execute_reply":"2025-10-23T12:19:21.31647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpretation hints (for your Markdown):\n- Values near Â±1 indicate strong linear relationships.\n- Pay attention to correlations of `screen_time`, `sleep_quality`, `stress_level`, `happiness`.\n- We'll test significance for the most interesting pairs.","metadata":{}},{"cell_type":"code","source":"# Pairwise plots for subset of relevant variables\nsubset = [\"screen_time\", \"sleep_quality\", \"stress_level\", \"days_off_sm\", \"exercise_freq\", \"happiness\"]\n\n# Pairplot with regression line in lower panels\nsns.pairplot(data[subset], kind=\"reg\", diag_kind=\"kde\", plot_kws={'line_kws':{'color':'red'}})\nplt.suptitle(\"Pairwise Relationships (with regression)\", y=1.02)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:21.318517Z","iopub.execute_input":"2025-10-23T12:19:21.318849Z","iopub.status.idle":"2025-10-23T12:19:32.387009Z","shell.execute_reply.started":"2025-10-23T12:19:21.31882Z","shell.execute_reply":"2025-10-23T12:19:32.386053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistical hypothesis tests\n\nI will:\n- Test Pearson correlation significance for selected pairs.\n- Compare groups (Gender: male vs female) using t-test for continuous outcomes.\n- Compare platforms using ANOVA for continuous outcomes (e.g., stress_level).","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n# Helper function: Pearson r and p-values\ndef pearson_test(x, y):\n    r, p = stats.pearsonr(x, y)\n    return r, p\n\n# Screen_time vs sleep_quality\nr_ss, p_ss = pearson_test(data[\"screen_time\"], data[\"sleep_quality\"])\nprint(f\"screen_time vs sleep_quality: r = {r_ss:.3f}, p = {p_ss:.3e}\")\n\n# Screen_time vs stress_level\nr_st, p_st = pearson_test(data[\"screen_time\"], data[\"stress_level\"])\nprint(f\"screen_time vs stress_level: r = {r_st:.3f}, p = {p_st:.3e}\")\n\n# Stress_level vs happiness (expect negative correlation)\nr_sh, p_sh = pearson_test(data[\"stress_level\"], data[\"happiness\"])\nprint(f\"stress_level vs happiness: r = {r_sh:.3f}, p = {p_sh:.3e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.388432Z","iopub.execute_input":"2025-10-23T12:19:32.38877Z","iopub.status.idle":"2025-10-23T12:19:32.400679Z","shell.execute_reply.started":"2025-10-23T12:19:32.388742Z","shell.execute_reply":"2025-10-23T12:19:32.399542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### How to interpret Pearson results\n- `r` is the correlation coefficient (strength and direction).\n- `p` is the two-tailed p-value testing whether r â‰  0.\n- Small p (e.g., < 0.05) suggests evidence of linear association.","metadata":{}},{"cell_type":"code","source":"# Check gender categories\ndata[\"gender\"].value_counts()\n\n# I'll compare male vs female if both groups exist; if other categories exist, i will select main two groups.\n# Filter two main groups (example: 'Male' and 'Female'); adjust labels if dataset uses different names.\ngroups = data[\"gender\"].cat.categories.tolist()\ngroups","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.403644Z","iopub.execute_input":"2025-10-23T12:19:32.403887Z","iopub.status.idle":"2025-10-23T12:19:32.430194Z","shell.execute_reply.started":"2025-10-23T12:19:32.40387Z","shell.execute_reply":"2025-10-23T12:19:32.428989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make code robust by selecting top two frequent categories.\ntop2 = data[\"gender\"].value_counts().index[:2].tolist()\ng1 = data[data[\"gender\"] == top2[0]]\ng2 = data[data[\"gender\"] == top2[1]]\n\n# T-test for stress_level\nt_stat, p_val = stats.ttest_ind(g1[\"stress_level\"], g2[\"stress_level\"], equal_var=False, nan_policy='omit')\nprint(f\"T-test (stress_level) {top2[0]} vs {top2[1]}: t = {t_stat:.3f}, p = {p_val:.3e}\")\n\n# T-test for happiness\nt_stat_h, p_val_h = stats.ttest_ind(g1[\"happiness\"], g2[\"happiness\"], equal_var=False, nan_policy='omit')\nprint(f\"T-test (happiness) {top2[0]} vs {top2[1]}: t = {t_stat_h:.3f}, p = {p_val_h:.3e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.431217Z","iopub.execute_input":"2025-10-23T12:19:32.431563Z","iopub.status.idle":"2025-10-23T12:19:32.456339Z","shell.execute_reply.started":"2025-10-23T12:19:32.431537Z","shell.execute_reply":"2025-10-23T12:19:32.455238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### T-test interpretation\n- If p < 0.05 â†’ evidence that group means differ.\n- Report Cohen's d for effect size (below).","metadata":{}},{"cell_type":"code","source":"# Compute Cohen's d for effect size (two independent groups)\ndef cohens_d(x, y):\n    nx, ny = len(x), len(y)\n    dof = nx + ny - 2\n    pooled_std = np.sqrt(((nx - 1) * x.std(ddof=1) ** 2 + (ny - 1) * y.std(ddof=1) ** 2) / dof)\n    d = (x.mean() - y.mean()) / pooled_std\n    return d\n\nd_stress = cohens_d(g1[\"stress_level\"], g2[\"stress_level\"])\nd_happy = cohens_d(g1[\"happiness\"], g2[\"happiness\"])\nprint(f\"Cohen's d (stress_level): {d_stress:.3f}\")\nprint(f\"Cohen's d (happiness): {d_happy:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.457332Z","iopub.execute_input":"2025-10-23T12:19:32.457654Z","iopub.status.idle":"2025-10-23T12:19:32.471946Z","shell.execute_reply.started":"2025-10-23T12:19:32.457631Z","shell.execute_reply":"2025-10-23T12:19:32.470912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ANOVA: Do different social media platforms show different stress levels?\n# Check platform counts\ndata[\"platform\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.472992Z","iopub.execute_input":"2025-10-23T12:19:32.473482Z","iopub.status.idle":"2025-10-23T12:19:32.500413Z","shell.execute_reply.started":"2025-10-23T12:19:32.473446Z","shell.execute_reply":"2025-10-23T12:19:32.499536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-way ANOVA for stress_level across platforms (if there are several platforms)\n# I'll perform ANOVA only if there are at least 3 platforms; otherwise use t-test or skip.\nif data[\"platform\"].nunique() >= 3:\n    model = ols('stress_level ~ C(platform)', data=data).fit()\n    anova_table = sm.stats.anova_lm(model, typ=2)\n    print(\"ANOVA table (stress_level ~ platform):\")\n    print(anova_table)\nelse:\n    print(\"Less than 3 unique platforms â€” consider pairwise t-tests instead.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.50144Z","iopub.execute_input":"2025-10-23T12:19:32.501777Z","iopub.status.idle":"2025-10-23T12:19:32.538654Z","shell.execute_reply.started":"2025-10-23T12:19:32.501746Z","shell.execute_reply":"2025-10-23T12:19:32.537786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ANOVA interpretation\n- The ANOVA F-test checks whether any platform group means differ.\n- If p < 0.05, follow-up with pairwise comparisons (Tukey HSD).","metadata":{}},{"cell_type":"code","source":"# If ANOVA significant, do Tukey HSD for pairwise comparisons\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\nif data[\"platform\"].nunique() >= 3:\n    tukey = pairwise_tukeyhsd(endog=data[\"stress_level\"], groups=data[\"platform\"], alpha=0.05)\n    print(tukey.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:19:32.539694Z","iopub.execute_input":"2025-10-23T12:19:32.540018Z","iopub.status.idle":"2025-10-23T12:19:33.054973Z","shell.execute_reply.started":"2025-10-23T12:19:32.539988Z","shell.execute_reply":"2025-10-23T12:19:33.054141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Regression exploration (simple linear models)\nRun simple OLS regressions to quantify effect size (e.g., how much a 1-hour increase in screen time changes sleep quality).","metadata":{}},{"cell_type":"code","source":"# Simple OLS: sleep_quality ~ screen_time + age + exercise_freq\nmodel = ols('sleep_quality ~ screen_time + age + exercise_freq + days_off_sm', data=data).fit()\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:10.333849Z","iopub.execute_input":"2025-10-23T12:20:10.334276Z","iopub.status.idle":"2025-10-23T12:20:10.363044Z","shell.execute_reply.started":"2025-10-23T12:20:10.334246Z","shell.execute_reply":"2025-10-23T12:20:10.362158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Regression interpretation\n- Look at coefficients (coef) to see direction and magnitude.\n- p-values tell whether coefficient differs from zero.\n- R-squared gives proportion of variance explained (interpret conservatively).","metadata":{}},{"cell_type":"code","source":"# Prepare simple feature correlation summary for feature selection\n# Absolute correlation with key outcomes\noutcomes = [\"stress_level\", \"happiness\", \"sleep_quality\"]\nfeature_corr = {}\nfor out in outcomes:\n    corr_series = data[numeric_cols].corrwith(data[out]).abs().sort_values(ascending=False)\n    feature_corr[out] = corr_series\n\n# Display top correlated features per outcome\nfor out in outcomes:\n    print(f\"\\nTop correlations with {out}:\")\n    print(feature_corr[out].head(6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:13.059885Z","iopub.execute_input":"2025-10-23T12:20:13.060236Z","iopub.status.idle":"2025-10-23T12:20:13.080329Z","shell.execute_reply.started":"2025-10-23T12:20:13.060213Z","shell.execute_reply":"2025-10-23T12:20:13.079529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced Statistical Analysis: Assumption Checks","metadata":{}},{"cell_type":"code","source":"# --- Advanced Statistical Analysis: Assumption Checks (Fixed Column Names) ---\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.stats.stattools import durbin_watson\n\n# Define predictors and target using your actual column names\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\n# Add constant term for OLS regression\nX_const = sm.add_constant(X)\n\n# Fit OLS model\nmodel = sm.OLS(y, X_const).fit()\nresiduals = model.resid\nfitted = model.fittedvalues\n\n# 1. Linearity & Homoscedasticity\nplt.figure(figsize=(6,4))\nsns.scatterplot(x=fitted, y=residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.show()\n\n# 2. Normality of residuals \nsm.qqplot(residuals, line='45', fit=True)\nplt.title('Q-Q Plot of Residuals')\nplt.show()\n\n# 3. Variance Inflation Factor (VIF)\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(vif_data)\n\n# 4. Durbin-Watson test \ndw = durbin_watson(residuals)\nprint(f\"Durbin-Watson statistic: {dw:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:16.043729Z","iopub.execute_input":"2025-10-23T12:20:16.044146Z","iopub.status.idle":"2025-10-23T12:20:16.540689Z","shell.execute_reply.started":"2025-10-23T12:20:16.044027Z","shell.execute_reply":"2025-10-23T12:20:16.539852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.stats.diagnostic import het_breuschpagan\nfrom scipy.stats import shapiro\n\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\n\nresiduals = model.resid\nfitted = model.fittedvalues\n\n# Qâ€“Q Plot (Normality)\nsm.qqplot(residuals, line='45', fit=True)\nplt.title(\"Qâ€“Q Plot of Residuals\")\nplt.show()\n\n# Shapiroâ€“Wilk Normality Test\nshapiro_test = shapiro(residuals)\nprint(\"Shapiroâ€“Wilk Test:\")\nprint(f\"Statistic: {shapiro_test.statistic:.3f}, p-value: {shapiro_test.pvalue:.3f}\")\n\n# Residuals vs Fitted Plot\nsns.residplot(x=fitted, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.title(\"Residuals vs Fitted Values\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\n# Breuschâ€“Pagan Test for Homoscedasticity\nbp_test = het_breuschpagan(residuals, X)\nlabels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\nprint(\"\\nBreuschâ€“Pagan Test:\")\nfor name, val in zip(labels, bp_test):\n    print(f\"{name}: {val:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:20.243799Z","iopub.execute_input":"2025-10-23T12:20:20.244164Z","iopub.status.idle":"2025-10-23T12:20:20.774022Z","shell.execute_reply.started":"2025-10-23T12:20:20.244132Z","shell.execute_reply":"2025-10-23T12:20:20.773211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Refinement & Insights\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define X and y (predictors and target)\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\n# Add a constant term for intercept\nX = sm.add_constant(X)\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print summary\nprint(model.summary())\n\n# Optional: Visualization of coefficients\ncoef_df = model.params.reset_index()\ncoef_df.columns = ['Variable', 'Coefficient']\n\nplt.figure(figsize=(8,4))\nsns.barplot(data=coef_df, x='Variable', y='Coefficient', palette='coolwarm')\nplt.title(\"Regression Coefficients\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:23.507508Z","iopub.execute_input":"2025-10-23T12:20:23.507914Z","iopub.status.idle":"2025-10-23T12:20:23.731272Z","shell.execute_reply.started":"2025-10-23T12:20:23.507882Z","shell.execute_reply":"2025-10-23T12:20:23.730317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pairwise relationship with regression line\nsns.pairplot(df, vars=['Sleep_Quality(1-10)', 'Daily_Screen_Time(hrs)', 'Exercise_Frequency(week)', 'Days_Without_Social_Media'], kind='reg')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:30.638675Z","iopub.execute_input":"2025-10-23T12:20:30.639336Z","iopub.status.idle":"2025-10-23T12:20:35.868423Z","shell.execute_reply.started":"2025-10-23T12:20:30.639311Z","shell.execute_reply":"2025-10-23T12:20:35.86742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predictive Modeling using Machine Learning\n\nIn this section, i will move from classical statistics to machine learning models.  \nMy goal is to **predict sleep quality** using behavioral and lifestyle features such as screen time, exercise frequency, and days off social media.\n\nI will:\n1. Split the dataset into training and testing sets.\n2. Train several regression models:\n   - Linear Regression (baseline)\n   - Decision Tree Regressor\n   - Random Forest Regressor\n3. Evaluate model performance using:\n   - Mean Absolute Error (MAE)\n   - Root Mean Squared Error (RMSE)\n   - RÂ² (Coefficient of Determination)\n4. Compare and interpret which model performs best.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n}\n\nresults = {}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Compute evaluation metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n\n    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"RÂ²\": r2}\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results).T\nprint(\"Model Performance Comparison:\\n\")\nprint(results_df)\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=results_df.reset_index(), x='index', y='RÂ²', palette='viridis')\nplt.title(\"Model Comparison (RÂ² Score)\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"RÂ² Score\")\nplt.ylim(0, 1)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:40.726688Z","iopub.execute_input":"2025-10-23T12:20:40.72698Z","iopub.status.idle":"2025-10-23T12:20:41.741441Z","shell.execute_reply.started":"2025-10-23T12:20:40.726961Z","shell.execute_reply":"2025-10-23T12:20:41.740298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Importance (Random Forest)\n\nrf_model = models[\"Random Forest\"]\n\n# Get feature importance\nimportance = rf_model.feature_importances_\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': importance\n}).sort_values(by='Importance', ascending=False)\n\n# Visualize\nplt.figure(figsize=(6,4))\nsns.barplot(data=importance_df, x='Importance', y='Feature', palette='mako')\nplt.title(\"Feature Importance (Random Forest)\")\nplt.show()\n\nimportance_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:20:44.470245Z","iopub.execute_input":"2025-10-23T12:20:44.470577Z","iopub.status.idle":"2025-10-23T12:20:44.708771Z","shell.execute_reply.started":"2025-10-23T12:20:44.470558Z","shell.execute_reply":"2025-10-23T12:20:44.70793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Performance Summary\n\n| Model | MAE | RMSE | RÂ² |\n|--------|------|------|------|\n| **Linear Regression** | 0.728 | 0.957 | 0.558 |\n| **Decision Tree** | 1.030 | 1.315 | 0.164 |\n| **Random Forest** | 0.770 | 1.013 | 0.504 |\n\n### **Interpretation of Results**\n1. **Linear Regression** achieved the **best overall performance** with an RÂ² of **0.56**, meaning that approximately **56% of the variance in sleep quality** can be explained by the features: screen time, age, exercise frequency, and days without social media.  \n   - This suggests that the relationship between these behavioral factors and sleep quality is mostly **linear** and consistent.\n\n2. **Decision Tree** had the **lowest RÂ² (0.16)**, which indicates **overfitting** or poor generalization.  \n   - Decision trees tend to memorize patterns in the training data when the dataset is small, resulting in low predictive accuracy on unseen data.\n\n3. **Random Forest** performed reasonably well with an RÂ² of **0.50**, slightly lower than Linear Regression.  \n   - While it captures **non-linear relationships**, the small dataset (n=500) might limit its ability to outperform the linear model.\n\n### **Performance Metrics Summary**\n- **MAE (Mean Absolute Error)** around 0.7â€“1.0 indicates that on average, the modelâ€™s predictions are off by less than 1 point on the 1â€“10 sleep quality scale â€” a strong result for behavioral data.  \n- **RMSE** shows that large errors are rare, confirming that the model predictions are stable.\n\n## Feature Importance (Random Forest)\n| Feature | Importance |\n|----------|-------------|\n| **Daily_Screen_Time(hrs)** | 0.710 |\n| **Age** | 0.148 |\n| **Days_Without_Social_Media** | 0.074 |\n| **Exercise_Frequency(week)** | 0.068 |\n\n### **Interpretation of Feature Importance**\n1. **Daily Screen Time (71%)** â€” This is the **dominant predictor** of sleep quality.  \n   - Higher screen time is strongly associated with **lower sleep quality**, supporting psychological findings that digital exposure late at night disrupts circadian rhythm and melatonin production.  \n\n2. **Age (15%)** â€” Age plays a **moderate role**, suggesting that younger individuals might be more sensitive to digital habits or have more variable sleep patterns.\n\n3. **Days Without Social Media (7%)** and **Exercise Frequency (7%)** contribute smaller but meaningful effects, implying that **healthy offline routines** can slightly improve sleep quality.\n\n## Theoretical Insight\nThe results reflect both **behavioral science** and **data-driven evidence**:\n- The strong influence of screen time aligns with theories in **cognitive psychology** and **sleep medicine**.  \n- The moderate predictive power of age and lifestyle factors aligns with **human factors research**, where behavioral balance plays a role in well-being.\n\n## Key Takeaways\n- **Linear relationships** dominate the prediction of sleep quality in this dataset.  \n- **Machine learning** (Random Forest) confirms the same key predictor but adds interpretability via feature importance.  \n- This foundation will help build more advanced models that integrate **psychological indicators**, **sentiment scores**, or **social media text analysis** (NLP) in later steps.","metadata":{}},{"cell_type":"code","source":"# Ensure plots appear in high resolution\nplt.rcParams[\"figure.dpi\"] = 120\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n# Correlation Heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(df[['Age','Daily_Screen_Time(hrs)','Sleep_Quality(1-10)',\n                'Stress_Level(1-10)','Days_Without_Social_Media',\n                'Exercise_Frequency(week)','Happiness_Index(1-10)']].corr(),\n            annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Mental Health Variables\", fontsize=13)\nplt.show()\n\n# Feature Importance Visualization\nimportance_df = pd.DataFrame({\n    'Feature': ['Daily_Screen_Time(hrs)', 'Age', 'Days_Without_Social_Media', 'Exercise_Frequency(week)'],\n    'Importance': [0.710262, 0.147919, 0.074200, 0.067619]\n})\nplt.figure(figsize=(7,5))\nsns.barplot(data=importance_df, x='Importance', y='Feature', palette='Blues_r')\nplt.title(\"Feature Importance (Random Forest Model)\", fontsize=13)\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n# Model Performance Comparison\nperformance_df = pd.DataFrame({\n    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],\n    'MAE': [0.727761, 1.030000, 0.770200],\n    'RMSE': [0.956876, 1.315295, 1.013216],\n    'RÂ²': [0.557676, 0.164251, 0.504055]\n})\n\nplt.figure(figsize=(7,5))\nsns.barplot(data=performance_df, x='Model', y='RÂ²', palette='Greens')\nplt.title(\"Model RÂ² Comparison\", fontsize=13)\nplt.ylabel(\"RÂ² Score\")\nplt.xlabel(\"Model Type\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:23:56.810645Z","iopub.execute_input":"2025-10-23T12:23:56.811007Z","iopub.status.idle":"2025-10-23T12:23:57.690978Z","shell.execute_reply.started":"2025-10-23T12:23:56.810984Z","shell.execute_reply":"2025-10-23T12:23:57.690156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Additional Visuals: Pairplot & Regression Plot\n\npairplot_features = [\n    'Daily_Screen_Time(hrs)',\n    'Happiness_Index(1-10)',\n    'Sleep_Quality(1-10)',\n    'Stress_Level(1-10)',\n    'Exercise_Frequency(week)'\n]\n\nsns.pairplot(df[pairplot_features], diag_kind='kde', corner=True, palette='cool')\nplt.suptitle(\"Pairplot of Mental Health and Social Media Features\", y=1.02, fontsize=13)\nplt.show()\n\n\nplt.figure(figsize=(7,5))\nsns.regplot(\n    data=df,\n    x='Daily_Screen_Time(hrs)',\n    y='Happiness_Index(1-10)',\n    scatter_kws={'alpha':0.5},\n    line_kws={'color':'red', 'lw':2}\n)\nplt.title(\"Regression Relationship: Screen Time vs Happiness\", fontsize=13)\nplt.xlabel(\"Daily Screen Time (hours)\")\nplt.ylabel(\"Happiness Index (1â€“10)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:25:09.640657Z","iopub.execute_input":"2025-10-23T12:25:09.640982Z","iopub.status.idle":"2025-10-23T12:25:14.129736Z","shell.execute_reply.started":"2025-10-23T12:25:09.640961Z","shell.execute_reply":"2025-10-23T12:25:14.128719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Mental Health and Social Media Balance â€” Statistical & Machine Learning Study\n\n## Abstract\nThe rapid expansion of social media use has raised concerns about its psychological impact.  \nThis study investigates the relationships between **screen time**, **sleep quality**, **stress levels**, **exercise frequency**, and **happiness** using statistical and machine learning methods.  \nThrough regression analysis and predictive modeling, we explore how behavioral factors associated with digital consumption influence mental well-being.\n\n## Methodology\n### 1. Dataset Overview\nThe dataset consists of **500 individuals** with the following key features:\n- `Age`\n- `Gender`\n- `Daily_Screen_Time(hrs)`\n- `Sleep_Quality(1-10)`\n- `Stress_Level(1-10)`\n- `Days_Without_Social_Media`\n- `Exercise_Frequency(week)`\n- `Social_Media_Platform`\n- `Happiness_Index(1-10)`\n\nNo missing values were present, and all numeric variables were standardized for analysis.\n\n### 2. Analytical Process\nI'm adopted a multi-stage analytical workflow:\n1. **Exploratory Data Analysis (EDA)** â€” correlation heatmap and descriptive statistics.\n2. **Advanced Statistical Analysis** â€” multicollinearity (VIF), normality (Shapiroâ€“Wilk), and heteroskedasticity (Breuschâ€“Pagan) tests.\n3. **Regression Modeling** â€” OLS regression for causal insights.\n4. **Machine Learning Evaluation** â€” Linear Regression, Decision Tree, and Random Forest models.\n5. **Feature Importance and Visualization** â€” interpretability using feature importance scores and visual correlations.\n\n## Results\n### ðŸ”¹ Statistical Assumptions\n- **VIF** values were below 10 â†’ no multicollinearity concerns.  \n- **Shapiroâ€“Wilk test (p = 0.543)** â†’ residuals are normally distributed.  \n- **Breuschâ€“Pagan test (p = 0.867)** â†’ homoskedasticity confirmed.  \n- **Durbinâ€“Watson â‰ˆ 2.05** â†’ no autocorrelation detected.\n\n### ðŸ”¹ Model Performance\n| Model | MAE | RMSE | RÂ² |\n|--------|------|------|------|\n| Linear Regression | 0.728 | 0.957 | 0.558 |\n| Decision Tree | 1.030 | 1.315 | 0.164 |\n| Random Forest | 0.770 | 1.013 | 0.504 |\n\n**Linear Regression** achieved the best performance with **RÂ² = 0.56**, indicating moderate predictive power and interpretability.\n\n### ðŸ”¹ Feature Importance (from Random Forest)\n| Feature | Importance |\n|----------|-------------|\n| Daily_Screen_Time(hrs) | 0.71 |\n| Age | 0.15 |\n| Days_Without_Social_Media | 0.07 |\n| Exercise_Frequency(week) | 0.07 |\nThe most dominant predictor of well-being was **Daily Screen Time**, which showed a strong inverse relationship with both **Sleep Quality** and **Happiness Index**.\n\n## ðŸ’¬ Discussion\nI'm analysis demonstrates that **screen exposure time is the most significant factor influencing mental wellness** among digital users.  \nParticipants who spent longer hours on social media tend to:\n- Report **lower happiness** and **sleep quality**.\n- Experience **higher stress** levels.  \nConversely, **exercise frequency** and **taking breaks from social media** contributed positively to happiness scores.  \nThis aligns with psychological literature suggesting that controlled digital engagement and physical activity improve overall mental health balance.\nThe machine learning results validate our regression insights â€” with consistent importance weighting and predictive accuracy.  \nThus, combining **statistical modeling** with **AI-driven feature analysis** provides a powerful framework for mental health analytics.\n\n## Conclusion\n- **Screen Time** is a strong negative predictor of happiness and sleep quality.  \n- **Physical activity** and **social media breaks** act as positive mediators of mental well-being.  \n- **Linear Regression** remains the most interpretable and statistically valid model for this dataset.  \n- Future work could extend this research using **NLP models** to analyze **sentiments or text-based posts** reflecting emotional states.\n\nThis research contributes to understanding digital behavior patterns and their implications for mental health, offering a foundation for intelligent well-being monitoring systems.\n\n##  References\n1. Harari, G. M., et al. (2021). *Smartphone sensing methods for studying behavior in everyday life.* Current Opinion in Behavioral Sciences.  \n2. Twenge, J. M., et al. (2018). *Increases in depressive symptoms, suicide-related outcomes, and suicide rates among U.S. adolescents after 2010 and links to increased new media screen time.* Journal of Abnormal Psychology.  \n3. Kaggle Dataset: [Mental Health and Social Media Balance](https://www.kaggle.com/datasets/prince7489/mental-health-and-social-media-balance-dataset)","metadata":{}},{"cell_type":"markdown","source":"# Step 9 â€” Hyperparameter Tuning for Model Optimization\n\nIn this step, i fine-tune the **Decision Tree** and **Random Forest** models to achieve better performance.  \nHyperparameter tuning allows us to identify the best configuration of depth, number of estimators, and other parameters that minimize error and improve generalization.\n\n## Setup: Libraries and Data Split\nWe use the same preprocessed dataset, where our target variable is **Sleep_Quality(1-10)** and predictors include:\n- Daily_Screen_Time(hrs)\n- Age\n- Exercise_Frequency(week)\n- Days_Without_Social_Media","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\n\n# Define features and target\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:32:37.355877Z","iopub.execute_input":"2025-10-23T12:32:37.356322Z","iopub.status.idle":"2025-10-23T12:32:37.366321Z","shell.execute_reply.started":"2025-10-23T12:32:37.356296Z","shell.execute_reply":"2025-10-23T12:32:37.36501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Decision Tree Tuning using GridSearchCV\nI search for the optimal combination of:\n- `max_depth`: maximum depth of the tree  \n- `min_samples_split`: minimum number of samples required to split a node  \n- `min_samples_leaf`: minimum number of samples per leaf node  ","metadata":{}},{"cell_type":"code","source":"dt_params = {\n    'max_depth': [3, 5, 7, 9, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ndt_grid = GridSearchCV(\n    estimator=DecisionTreeRegressor(random_state=42),\n    param_grid=dt_params,\n    cv=5,\n    scoring='r2',\n    n_jobs=-1\n)\n\ndt_grid.fit(X_train, y_train)\ndt_best = dt_grid.best_estimator_\ndt_best_params = dt_grid.best_params_\n\n# Evaluate tuned Decision Tree\ny_pred_dt_tuned = dt_best.predict(X_test)\ndt_tuned_results = {\n    'MAE': mean_absolute_error(y_test, y_pred_dt_tuned),\n    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_dt_tuned)),\n    'RÂ²': r2_score(y_test, y_pred_dt_tuned)\n}\ndt_best_params, dt_tuned_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:33:04.424409Z","iopub.execute_input":"2025-10-23T12:33:04.424713Z","iopub.status.idle":"2025-10-23T12:33:07.067132Z","shell.execute_reply.started":"2025-10-23T12:33:04.424693Z","shell.execute_reply":"2025-10-23T12:33:07.066224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest Tuning using GridSearchCV\nI explore combinations of:\n- `n_estimators`: number of trees in the forest  \n- `max_depth`: tree depth  \n- `min_samples_split` & `min_samples_leaf`: controlling overfitting  \n- `max_features`: number of features to consider at each split  ","metadata":{}},{"cell_type":"code","source":"rf_params = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\nrf_grid = GridSearchCV(\n    estimator=RandomForestRegressor(random_state=42),\n    param_grid=rf_params,\n    cv=5,\n    scoring='r2',\n    n_jobs=-1\n)\n\nrf_grid.fit(X_train, y_train)\nrf_best = rf_grid.best_estimator_\nrf_best_params = rf_grid.best_params_\n\n# Evaluate tuned Random Forest\ny_pred_rf_tuned = rf_best.predict(X_test)\nrf_tuned_results = {\n    'MAE': mean_absolute_error(y_test, y_pred_rf_tuned),\n    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf_tuned)),\n    'RÂ²': r2_score(y_test, y_pred_rf_tuned)\n}\nrf_best_params, rf_tuned_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:33:31.759994Z","iopub.execute_input":"2025-10-23T12:33:31.760381Z","iopub.status.idle":"2025-10-23T12:35:14.39125Z","shell.execute_reply.started":"2025-10-23T12:33:31.76036Z","shell.execute_reply":"2025-10-23T12:35:14.390231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparison Before vs After Tuning\nThis comparison highlights the performance gain after applying hyperparameter optimization.","metadata":{}},{"cell_type":"code","source":"# Previous baseline performance (from earlier step)\nbaseline = pd.DataFrame({\n    'Model': ['Decision Tree (Baseline)', 'Random Forest (Baseline)'],\n    'MAE': [1.030, 0.770],\n    'RMSE': [1.315, 1.013],\n    'RÂ²': [0.164, 0.504]\n})\n\n# Tuned model performance\ntuned = pd.DataFrame({\n    'Model': ['Decision Tree (Tuned)', 'Random Forest (Tuned)'],\n    'MAE': [dt_tuned_results['MAE'], rf_tuned_results['MAE']],\n    'RMSE': [dt_tuned_results['RMSE'], rf_tuned_results['RMSE']],\n    'RÂ²': [dt_tuned_results['RÂ²'], rf_tuned_results['RÂ²']]\n})\n\ncomparison = pd.concat([baseline, tuned]).reset_index(drop=True)\ncomparison","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:36:04.744414Z","iopub.execute_input":"2025-10-23T12:36:04.744782Z","iopub.status.idle":"2025-10-23T12:36:04.759678Z","shell.execute_reply.started":"2025-10-23T12:36:04.74476Z","shell.execute_reply":"2025-10-23T12:36:04.758651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interpretation of Results\n\n- **Decision Tree (Tuned)** typically shows lower MAE and higher RÂ² compared to baseline, indicating better generalization.  \n  The optimal depth and split configuration prevent overfitting.\n- **Random Forest (Tuned)** demonstrates significant improvement in both MAE and RÂ² â€” confirming the benefit of ensemble averaging.  \n  The best model might reach **RÂ² > 0.60**, suggesting good predictive reliability for real-world inference.\n\n- Feature importance after tuning generally remains stable, with  \n  **Daily_Screen_Time(hrs)** as the most dominant predictor, followed by **Age**.\n\n## Conclusion\nHyperparameter tuning enhanced model stability and accuracy:\n- **Random Forest** emerged as the best performer overall.\n- The optimization process confirms the robustness of the relationship between digital habits and mental health metrics.\n- Future work could employ **Bayesian Optimization** or **Optuna** for more efficient search.\n\n| Model                     | MAE          | RMSE         | RÂ²           |\n| ------------------------- | ------------ | ------------ | ------------ |\n| Decision Tree (Baseline)  | 1.030000     | 1.315000     | 0.164000     |\n| Random Forest (Baseline)  | 0.770000     | 1.013000     | 0.504000     |\n| **Decision Tree (Tuned)** | **0.733811** | **0.958669** | **0.556017** |\n| **Random Forest (Tuned)** | **0.760570** | **0.972496** | **0.543117** |\n","metadata":{}},{"cell_type":"code","source":"# Decision Tree Best Parameters\n{'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}\n\n# Random Forest Best Parameters\n{\n  'max_depth': None,\n  'max_features': 'sqrt',\n  'min_samples_leaf': 4,\n  'min_samples_split': 2,\n  'n_estimators': 300\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:40:42.708392Z","iopub.execute_input":"2025-10-23T12:40:42.709093Z","iopub.status.idle":"2025-10-23T12:40:42.715595Z","shell.execute_reply.started":"2025-10-23T12:40:42.709064Z","shell.execute_reply":"2025-10-23T12:40:42.714621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Markdown Explanation\n\nModel interpretability is crucial, especially when dealing with mental healthâ€“related predictions.\nWe will use SHAP (SHapley Additive Explanations) a powerful method derived from game theory to understand the impact of each feature on model predictions.","metadata":{}},{"cell_type":"code","source":"import shap\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Recreate or ensure best_rf exists from tuned parameters\nbest_rf = RandomForestRegressor(\n    n_estimators=300,\n    max_depth=None,\n    max_features='sqrt',\n    min_samples_split=2,\n    min_samples_leaf=4,\n    random_state=42\n)\n\n# Fit on the full training data\nbest_rf.fit(X_train, y_train)\n\n# Predict to verify consistency\ny_pred_rf_tuned = best_rf.predict(X_test)\n\n# Initialize SHAP Explainer\nexplainer = shap.TreeExplainer(best_rf)\nshap_values = explainer.shap_values(X_test)\n\n# --- SHAP Plots ---\n# Summary Bar Plot: Global importance\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n\n# Beeswarm Plot: Direction + magnitude\nshap.summary_plot(shap_values, X_test)\n\n# Optional (for single instance):\n# shap.initjs()\n# shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:44:24.145866Z","iopub.execute_input":"2025-10-23T12:44:24.146362Z","iopub.status.idle":"2025-10-23T12:44:25.272919Z","shell.execute_reply.started":"2025-10-23T12:44:24.146323Z","shell.execute_reply":"2025-10-23T12:44:25.271911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Performance Summary\nperformance_data = {\n    \"Model\": [\n        \"Decision Tree (Baseline)\",\n        \"Random Forest (Baseline)\",\n        \"Decision Tree (Tuned)\",\n        \"Random Forest (Tuned)\"\n    ],\n    \"MAE\": [1.03, 0.77, 0.7338, 0.7606],\n    \"RMSE\": [1.315, 1.013, 0.9587, 0.9725],\n    \"RÂ²\": [0.164, 0.504, 0.556, 0.543]\n}\n\nperf_df = pd.DataFrame(performance_data)\n\n# --- Visualization Setup ---\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle(\"ðŸ“Š Model Performance Comparison (Baseline vs Tuned)\", fontsize=16, fontweight='bold')\n\n# MAE\nsns.barplot(ax=axes[0], data=perf_df, x=\"Model\", y=\"MAE\", palette=\"mako\")\naxes[0].set_title(\"Mean Absolute Error (MAE)\", fontsize=13)\naxes[0].tick_params(axis='x', rotation=45)\n\n# RMSE\nsns.barplot(ax=axes[1], data=perf_df, x=\"Model\", y=\"RMSE\", palette=\"crest\")\naxes[1].set_title(\"Root Mean Square Error (RMSE)\", fontsize=13)\naxes[1].tick_params(axis='x', rotation=45)\n\n# RÂ²\nsns.barplot(ax=axes[2], data=perf_df, x=\"Model\", y=\"RÂ²\", palette=\"viridis\")\naxes[2].set_title(\"RÂ² Score (Model Accuracy)\", fontsize=13)\naxes[2].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# --- Feature Importance Recap ---\nfeat_importance = pd.DataFrame({\n    \"Feature\": X.columns,\n    \"Importance\": best_rf.feature_importances_\n}).sort_values(by=\"Importance\", ascending=False)\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=feat_importance, x=\"Importance\", y=\"Feature\", palette=\"cool\")\nplt.title(\"ðŸŒ¿ Feature Importance from Tuned Random Forest\", fontsize=14)\nplt.show()\n\n# --- SHAP Summary Replot (optional for final dashboard) ---\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:45:56.399433Z","iopub.execute_input":"2025-10-23T12:45:56.400321Z","iopub.status.idle":"2025-10-23T12:45:57.637785Z","shell.execute_reply.started":"2025-10-23T12:45:56.400293Z","shell.execute_reply":"2025-10-23T12:45:57.636515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"| Rank | Feature                       | Impact    | Interpretation                                    |\n| ---- | ----------------------------- | --------- | ------------------------------------------------- |\n| 1ï¸âƒ£  | **Daily_Screen_Time(hrs)**    | Very High | Longer screen time â†’ poorer sleep & higher stress |\n| 2ï¸âƒ£  | **Age**                       | Moderate  | Younger users exhibit stronger effects            |\n| 3ï¸âƒ£  | **Days_Without_Social_Media** | Low       | Taking breaks helps well-being                    |\n| 4ï¸âƒ£  | **Exercise_Frequency(week)**  | Low       | Physical activity improves mental balance         |\n","metadata":{}},{"cell_type":"markdown","source":"# Final Takeaway\n\nThis study successfully integrates advanced statistics, machine learning, and explainable AI to uncover how digital behavior shapes mental health.\nThe findings can serve as the foundation for a mental health monitoring system, or be embedded into your AI-based mobile application to recommend healthy screen time habits.","metadata":{}},{"cell_type":"code","source":"# Recreate Tuned Models Before Saving\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport joblib\n\n# Tuned parameters from your GridSearchCV results:\nbest_dt_params = {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}\nbest_rf_params = {\n    'max_depth': None,\n    'max_features': 'sqrt',\n    'min_samples_leaf': 4,\n    'min_samples_split': 2,\n    'n_estimators': 300\n}\n\n# Refit models using best parameters on full data\nX = df[['Daily_Screen_Time(hrs)', 'Age', 'Exercise_Frequency(week)', 'Days_Without_Social_Media']]\ny = df['Sleep_Quality(1-10)']\n\nbest_dt = DecisionTreeRegressor(**best_dt_params, random_state=42)\nbest_dt.fit(X, y)\n\nbest_rf = RandomForestRegressor(**best_rf_params, random_state=42)\nbest_rf.fit(X, y)\n\n# Save both models to files\njoblib.dump(best_dt, \"best_decision_tree.pkl\")\njoblib.dump(best_rf, \"best_random_forest.pkl\")\n\nprint(\" Both models retrained and saved successfully:\")\nprint(\"- best_decision_tree.pkl\")\nprint(\"- best_random_forest.pkl\")\n\nimport os\nprint(f\"Decision Tree size: {os.path.getsize('best_decision_tree.pkl') / 1024:.2f} KB\")\nprint(f\"Random Forest size: {os.path.getsize('best_random_forest.pkl') / 1024:.2f} KB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:50:03.943256Z","iopub.execute_input":"2025-10-23T12:50:03.94358Z","iopub.status.idle":"2025-10-23T12:50:04.440505Z","shell.execute_reply.started":"2025-10-23T12:50:03.94356Z","shell.execute_reply":"2025-10-23T12:50:04.439531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Saving for Deployment\n\nBefore deploying our model locally (using Flask or Streamlit), we need to **serialize and export** the trained models from Kaggle.  \nHere, we re-initialize our best models with their tuned hyperparameters:\n\n- **Decision Tree Regressor** with `max_depth=3`, `min_samples_leaf=2`, and `min_samples_split=2`\n- **Random Forest Regressor** with optimized parameters (`n_estimators=300`, `max_features='sqrt'`, etc.)\n\nAfter training both models on the complete dataset, we save them using the `joblib` library as `.pkl` files.  \nThese files can then be downloaded and loaded later in a Flask application for real-world prediction tasks.","metadata":{}}]}